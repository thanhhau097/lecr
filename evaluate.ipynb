{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa54d8-22bc-4134-9fff-bf6c8120ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbebda6-bd0f-418f-92f4-a2839c8155b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def decontracted(phrase):\n",
    "\n",
    "    # Specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    # ..\n",
    "\n",
    "    # General\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    # ..\n",
    "\n",
    "    return phrase\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in list(string.punctuation): text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "def clean_number(text):\n",
    "    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n",
    "    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n",
    "    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n",
    "    return text\n",
    "\n",
    "def clean_whitespace(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_repeat_words(text):\n",
    "    return re.sub(r\"(\\w*)(\\w)\\2(\\w*)\", r\"\\1\\2\\3\", text)\n",
    "\n",
    "def clean_text(text):\n",
    "    # text_blob = TextBlob(text)\n",
    "    # text = str(text_blob.correct())\n",
    "    text = str(text)\n",
    "    text = decontracted(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = clean_number(text)\n",
    "    text = clean_whitespace(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a1dd2-a3b5-40ec-87e6-4231794a3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from dataset import AutoTokenizer, LANGUAGE_TOKENS, CATEGORY_TOKENS, LEVEL_TOKENS, KIND_TOKENS, OTHER_TOKENS\n",
    "from model import Model\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45499b1d-2636-4ffd-9d4f-e578ce45e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "TEST_MODE = False\n",
    "\n",
    "# --------------------- VALIDATION SET --------------------------\n",
    "from tqdm import tqdm\n",
    "if not TEST_MODE:\n",
    "    data_df = pd.read_csv(\"./data/siamese_train.csv\")\n",
    "    fold = 0\n",
    "\n",
    "data_folder = Path(\"./data\")\n",
    "# TODO: we have to process for test set ourselves\n",
    "contents_df = pd.read_csv(data_folder/'content.csv')\n",
    "contents_df = contents_df.fillna('')\n",
    "contents_df['title_len'] = contents_df.title.str.len()\n",
    "contents_df = contents_df.sort_values(by='title_len', axis=0).reset_index(drop=True).drop(columns=['title_len'])\n",
    "topics_df = pd.read_csv(data_folder/'topics.csv')\n",
    "topics_df = topics_df.fillna('')\n",
    "topics_df['title_len'] = topics_df.title.str.len()\n",
    "topics_df = topics_df.sort_values(by='title_len', axis=0).reset_index(drop=True).drop(columns=['title_len'])\n",
    "subs_df = pd.read_csv(data_folder/'sample_submission.csv')\n",
    "corrs_df = pd.read_csv(data_folder/'correlations.csv')\n",
    "\n",
    "\n",
    "topics_df[\"title\"] = topics_df[\"title\"].apply(clean_text)\n",
    "topics_df[\"description\"] = topics_df[\"description\"].apply(clean_text)\n",
    "\n",
    "contents_df[\"title\"] = contents_df[\"title\"].apply(clean_text)\n",
    "contents_df[\"description\"] = contents_df[\"description\"].apply(clean_text)\n",
    "contents_df[\"text\"] = contents_df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82366c3e-915e-4dc6-9926-305514f27a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "if TEST_MODE:\n",
    "    topics_df = topics_df[topics_df.id.isin(subs_df.topic_id)]\n",
    "else: # VAL_MODE\n",
    "    topics_df = topics_df[topics_df.id.isin(data_df[data_df[\"fold\"] == fold].topics_ids)]\n",
    "\n",
    "topic_dict = {}\n",
    "for i, (index, row) in tqdm(enumerate(topics_df.iterrows())):\n",
    "    text = \"<|topic|>\" + f\"<|lang_{row['language']}|>\" + f\"<|category_{row['category']}|>\" + f\"<|level_{row['level']}|>\"\n",
    "    text += \"<s_title>\" + row[\"title\"] + \"</s_title>\" + \"<s_description>\" + row[\"description\"] + \"</s_description>\"\n",
    "    topic_dict[row[\"id\"]] = text\n",
    "\n",
    "content_dict = {}\n",
    "for i, (index, row) in tqdm(enumerate(contents_df.iterrows())):\n",
    "    text = \"<|content|>\" + f\"<|lang_{row['language']}|>\" + f\"<|kind_{row['kind']}|>\"\n",
    "    text += \"<s_title>\" + row[\"title\"] + \"</s_title>\" + \"<s_description>\" + row[\"description\"] + \"</s_description>\" + \"<s_text>\" + row[\"text\"] + \"</s_text>\"\n",
    "    content_dict[row[\"id\"]] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2782f1a-36d1-4b5a-89b6-926aeed6541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df[\"topic_text\"] = topic_dict.values()\n",
    "topics_df[\"topic_text\"] = topics_df[\"topic_text\"].apply(lambda x: x[:2048])\n",
    "\n",
    "contents_df[\"content_text\"] = content_dict.values()\n",
    "contents_df[\"content_text\"] = contents_df[\"content_text\"].apply(lambda x: x[:2048])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd58ee0-c996-4b14-9c3b-7d22847dc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer_name='xlm-roberta-base', max_len=512):\n",
    "        self.texts = texts\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.tokenizer.add_special_tokens(dict(additional_special_tokens=LANGUAGE_TOKENS + CATEGORY_TOKENS + LEVEL_TOKENS + KIND_TOKENS + OTHER_TOKENS))\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        \n",
    "        # topic\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text, \n",
    "            return_tensors = None, \n",
    "            add_special_tokens = True, \n",
    "            max_length = self.max_len,\n",
    "            pad_to_max_length = True,\n",
    "            truncation = True\n",
    "        )\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "            \n",
    "        return inputs\n",
    "    \n",
    "def collate_fn(inputs):\n",
    "    inputs = default_collate(inputs)\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09637446-9ae8-4a8b-b60e-629097b0d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dataset = InferenceDataset(texts=list(topics_df.topic_text.values))\n",
    "topic_dataloader = DataLoader(topic_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d32b1-318b-4424-af01-bbf075298558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Model(tokenizer=topic_dataset.tokenizer, model_name=\"xlm-roberta-base\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "weights_path = \"outputs/checkpoint-40000/pytorch_model.bin\"\n",
    "\n",
    "state_dict = torch.load(weights_path)\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    new_state_dict[k[6:]] = v\n",
    "\n",
    "model.model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f33aac-6b81-4c11-9855-4e627d36b2ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_embs = []\n",
    "\n",
    "for inputs in tqdm(topic_dataloader):\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k].to(device)\n",
    "    out = model.feature(inputs)\n",
    "    topic_embs.extend(out.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f63849-09ae-43c5-bb5b-1bdbcba33f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dataset = InferenceDataset(texts=list(contents_df.content_text.values))\n",
    "content_dataloader = DataLoader(content_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "content_embs = []\n",
    "\n",
    "for inputs in tqdm(content_dataloader):\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k].to(device)\n",
    "    out = model.feature(inputs)\n",
    "    content_embs.extend(out.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a621519-5667-4375-bfaf-4212315d6330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from saved files\n",
    "# torch.save(topic_embs, \"./data/topic_embs.pt\")\n",
    "# torch.save(content_embs, \"./data/content_embs.pt\")\n",
    "\n",
    "# topic_embs = torch.load(\"./data/topic_embs.pt\")\n",
    "# content_embs = torch.load(\"./data/content_embs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6823d481-81c7-4038-be02-4cced8eee0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "\n",
    "content_forest = AnnoyIndex(content_embs[0].shape[0], metric='angular')\n",
    "for i, item in tqdm(enumerate(content_embs), total=len(content_embs)):\n",
    "    content_forest.add_item(i, item)\n",
    "content_forest.build(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda876a8-cfb2-4697-88d7-e313f3a1669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics = topics_df[topics_df.has_content==True][['id', 'title', 'language']].reset_index(drop=True)\n",
    "\n",
    "topics = topics_df\n",
    "\n",
    "test = topics\n",
    "all_content_ids = contents_df.id.to_numpy()\n",
    "all_content_titles = contents_df.title.to_numpy()\n",
    "all_content_language = contents_df.language.to_numpy()\n",
    "all_test_ids = list(topics.id)\n",
    "all_test_title = list(topics.title)\n",
    "all_test_language = list(test.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a11b16-c327-4ce4-9b38-f3e3bec2d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_forest.get_nns_by_vector(topic_embs[5], 10, include_distances=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f1349-3f46-45e2-9973-42e300538bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4596424-e963-48fd-8f4a-627e77f82c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find by distance instead\n",
    "\n",
    "nearest_content_count = 200\n",
    "fuzzy_filter = 0\n",
    "\n",
    "preds = []\n",
    "for i, t_e in tqdm(enumerate(topic_embs), total=len(topic_embs), desc=f'Getting Preds'):\n",
    "    indexes, distances = content_forest.get_nns_by_vector(t_e, nearest_content_count, include_distances=True)\n",
    "    indexes = [i for i, d in zip(indexes, distances) if d < 10]\n",
    "    \n",
    "    topic_id = all_test_ids[i]\n",
    "    topic_text = all_test_title[i]\n",
    "    topic_lang = all_test_language[i]\n",
    "    \n",
    "    filtered_indexes = []\n",
    "    for idx in indexes:\n",
    "        if topic_lang != all_content_language[idx]:\n",
    "            continue\n",
    "        fuzzy_value = fuzz.token_set_ratio(all_content_titles[idx], topic_text)\n",
    "        if fuzzy_value > fuzzy_filter:\n",
    "            filtered_indexes.append(idx)\n",
    "\n",
    "    if len(filtered_indexes) == 0:\n",
    "        indexes = list(set(filtered_indexes + indexes[:8-len(filtered_indexes)]))\n",
    "    else:\n",
    "        indexes = filtered_indexes[:10]\n",
    "    content_ids = all_content_ids[indexes]\n",
    "    preds.append({\n",
    "        'topic_id': topic_id,\n",
    "        'content_ids': ' '.join(content_ids)\n",
    "    })\n",
    "preds = pd.DataFrame.from_records(preds)\n",
    "\n",
    "preds.to_csv('submission.csv', index=False)\n",
    "\n",
    "if not TEST_MODE:\n",
    "    from engine import f2_score\n",
    "    gt = corrs_df[corrs_df.topic_id.isin(data_df[(data_df[\"fold\"] == fold)].topics_ids)].sort_values(\"topic_id\")    \n",
    "    preds = preds.sort_values(\"topic_id\")    \n",
    "    print(\"f2_score\", f2_score(gt[\"content_ids\"], preds[\"content_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a966351-eba8-448e-b573-bde38ddd34e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053aba78-c7db-40b2-91ad-0ec6acf45270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e9b54f-c4d3-48be-bb7f-80eaa30e3c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
