{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa54d8-22bc-4134-9fff-bf6c8120ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install annoy\n",
    "# import os\n",
    "# # print(os.environ[\"LD_LIBRARY_PATH\"])\n",
    "# os.environ[\"LD_LIBRARY_PATH\"] = \"/opt/conda/lib/python3.8/site-packages/torch/lib:/usr/local/cuda-11.3/lib64\"\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ed7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LD_LIBRARY_PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f97985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"LD_LIBRARY_PATH\"] = \"/opt/conda/lib/:/usr/local/cuda-11.3/lib64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbebda6-bd0f-418f-92f4-a2839c8155b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text\n",
    "# from textblob import TextBlob\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def decontracted(phrase):\n",
    "\n",
    "    # Specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    # ..\n",
    "\n",
    "    # General\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    # ..\n",
    "\n",
    "    return phrase\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in list(string.punctuation): text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "def clean_number(text):\n",
    "    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n",
    "    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n",
    "    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n",
    "    return text\n",
    "\n",
    "def clean_whitespace(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_repeat_words(text):\n",
    "    return re.sub(r\"(\\w*)(\\w)\\2(\\w*)\", r\"\\1\\2\\3\", text)\n",
    "\n",
    "def clean_text(text):\n",
    "    # text_blob = TextBlob(text)\n",
    "    # text = str(text_blob.correct())\n",
    "    text = str(text)\n",
    "    text = decontracted(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = clean_number(text)\n",
    "    text = clean_whitespace(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a1dd2-a3b5-40ec-87e6-4231794a3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from dataset import AutoTokenizer, LANGUAGE_TOKENS, CATEGORY_TOKENS, LEVEL_TOKENS, KIND_TOKENS, OTHER_TOKENS, RELATION_TOKENS\n",
    "from model import Model\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45499b1d-2636-4ffd-9d4f-e578ce45e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "TEST_MODE = False\n",
    "\n",
    "# --------------------- VALIDATION SET --------------------------\n",
    "from tqdm import tqdm\n",
    "if not TEST_MODE:\n",
    "    data_df = pd.read_csv(\"./data/supervised_correlations.csv\")\n",
    "    fold = 0\n",
    "val_topic_ids = set(list(data_df[data_df[\"fold\"] == fold].topics_ids))\n",
    "# del data_df\n",
    "\n",
    "data_folder = Path(\"./data\")\n",
    "# TODO: we have to process for test set ourselves\n",
    "contents_df = pd.read_csv(data_folder/'content.csv')\n",
    "contents_df = contents_df.fillna('')\n",
    "contents_df['title_len'] = contents_df.title.str.len()\n",
    "contents_df = contents_df.sort_values(by='title_len', axis=0).reset_index(drop=True).drop(columns=['title_len'])\n",
    "topics_df = pd.read_csv(data_folder/'topics.csv')\n",
    "topics_df = topics_df.fillna('')\n",
    "topics_df['title_len'] = topics_df.title.str.len()\n",
    "topics_df = topics_df.sort_values(by='title_len', axis=0).reset_index(drop=True).drop(columns=['title_len'])\n",
    "subs_df = pd.read_csv(data_folder/'sample_submission.csv')\n",
    "corrs_df = pd.read_csv(data_folder/'correlations.csv')\n",
    "\n",
    "\n",
    "topics_df[\"title\"] = topics_df[\"title\"].apply(clean_text)\n",
    "topics_df[\"description\"] = topics_df[\"description\"].apply(clean_text)\n",
    "\n",
    "contents_df[\"title\"] = contents_df[\"title\"].apply(clean_text)\n",
    "contents_df[\"description\"] = contents_df[\"description\"].apply(clean_text)\n",
    "# contents_df[\"text\"] = contents_df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7573915-e417-4520-9e69-3d08bc5f998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_topic_ids = set(val_topic_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised_correlations = pd.read_csv(\"data/supervised_correlations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02882b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised_correlations[(supervised_correlations[\"topics_ids\"].isin(val_topic_ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ea076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised_correlations[(supervised_correlations[\"topics_ids\"].isin(val_topic_ids)) & (supervised_correlations[\"target\"] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126331e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "sep_token = tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82366c3e-915e-4dc6-9926-305514f27a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "topic_df = topics_df\n",
    "content_df = contents_df\n",
    "# parent and children information\n",
    "parents = defaultdict(lambda: [])\n",
    "children = defaultdict(lambda: [])\n",
    "topic_title_dict = {}\n",
    "\n",
    "all_topic_ids = set(topic_df.id.values)\n",
    "for i, row in tqdm(topic_df.iterrows()):\n",
    "    if row[\"parent\"] in all_topic_ids:\n",
    "        parents[row[\"id\"]].append(row[\"parent\"])\n",
    "        children[row[\"parent\"]].append(row[\"id\"])\n",
    "\n",
    "    topic_title_dict[row[\"id\"]] = row[\"title\"]\n",
    "\n",
    "# get concatenated texts\n",
    "topic_dict = {}\n",
    "for i, (index, row) in tqdm(enumerate(topic_df.iterrows())):\n",
    "    text = (\n",
    "        \"<|topic|>\"\n",
    "        + f\"<|lang_{row['language']}|>\"\n",
    "        + f\"<|category_{row['category']}|>\"\n",
    "        + f\"<|level_{row['level']}|>\"\n",
    "    )\n",
    "    text += (\n",
    "        \"<s_title>\"\n",
    "        + row[\"title\"]\n",
    "        + \"</s_title>\"\n",
    "        + \"<s_description>\"\n",
    "        + row[\"description\"]\n",
    "        + \"</s_description>\"\n",
    "    )\n",
    "\n",
    "    context_text = \"<s_parent>\" \n",
    "    max_successor = 10\n",
    "    parent_id = parents.get(row[\"id\"], [None])[0]\n",
    "\n",
    "    i = 0\n",
    "    while parent_id and i < max_successor:\n",
    "        context_text += topic_title_dict[parent_id] + sep_token\n",
    "        parent_id = parents.get(parent_id, [None])[0]\n",
    "        i += 1\n",
    "\n",
    "    context_text += \"</s_parent>\"\n",
    "\n",
    "    if children.get(row[\"id\"]):\n",
    "        children_text = \"<s_children>\"\n",
    "        for child_topic_id in children.get(row[\"id\"]):\n",
    "            children_text += topic_title_dict[child_topic_id] + sep_token\n",
    "        children_text = children_text[:-(len(sep_token))] + \"</s_children>\"\n",
    "    else:\n",
    "        children_text = \"\"\n",
    "\n",
    "    context_text += children_text\n",
    "    topic_dict[row[\"id\"]] = text + context_text\n",
    "\n",
    "content_dict = {}\n",
    "for i, (index, row) in tqdm(enumerate(content_df.iterrows())):\n",
    "    text = \"<|content|>\" + f\"<|lang_{row['language']}|>\" + f\"<|kind_{row['kind']}|>\"\n",
    "    text += (\n",
    "        \"<s_title>\"\n",
    "        + row[\"title\"]\n",
    "        + \"</s_title>\"\n",
    "        + \"<s_description>\"\n",
    "        + row[\"description\"]\n",
    "        + \"</s_description>\"\n",
    "        + \"<s_text>\" + str(row[\"text\"]) + \"</s_text>\"\n",
    "    )\n",
    "    content_dict[row[\"id\"]] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebda367-a116-4737-959b-51c0b6c99a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df # [topics_df.id == \"t_b68dd8c98746\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2782f1a-36d1-4b5a-89b6-926aeed6541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df[\"topic_text\"] = topic_dict.values()\n",
    "topics_df[\"topic_text\"] = topics_df[\"topic_text\"] # .apply(lambda x: x[:2048])\n",
    "\n",
    "contents_df[\"content_text\"] = content_dict.values()\n",
    "contents_df[\"content_text\"] = contents_df[\"content_text\"] # .apply(lambda x: x[:2048])\n",
    "\n",
    "if TEST_MODE:\n",
    "    topics_df = topics_df[topics_df.id.isin(subs_df.topic_id)]\n",
    "else: # VAL_MODE\n",
    "    topics_df = topics_df[topics_df.id.isin(val_topic_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfc3ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([item for item in topics_df.topic_text.values if \"<s_parent>\" in item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd58ee0-c996-4b14-9c3b-7d22847dc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import init_tokenizer\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer_name='xlm-roberta-base', max_len=512):\n",
    "        self.texts = texts\n",
    "\n",
    "        self.tokenizer = init_tokenizer(tokenizer_name)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        \n",
    "        # topic\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text, \n",
    "            return_tensors = None, \n",
    "            add_special_tokens = True, \n",
    "            max_length = self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation = True\n",
    "        )\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "            \n",
    "        return inputs\n",
    "    \n",
    "def collate_fn(inputs):\n",
    "    inputs = default_collate(inputs)\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09637446-9ae8-4a8b-b60e-629097b0d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dataset = InferenceDataset(texts=list(topics_df.topic_text.values), tokenizer_name='sentence-transformers/all-MiniLM-L6-v2', max_len=128)\n",
    "topic_dataloader = DataLoader(topic_dataset, num_workers=16, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d32b1-318b-4424-af01-bbf075298558",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Model(tokenizer_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_name=\"sentence-transformers/all-MiniLM-L6-v2\", objective=\"siamese\", is_sentence_transformers=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "weights_path = \"./data/siamese_model_fold0_0.82832.pth\"\n",
    "\n",
    "state_dict = torch.load(weights_path)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f33aac-6b81-4c11-9855-4e627d36b2ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "topic_embs = []\n",
    "\n",
    "for inputs in tqdm(topic_dataloader):\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k].to(device)\n",
    "    out = model.feature(inputs)\n",
    "    topic_embs.extend(out.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f63849-09ae-43c5-bb5b-1bdbcba33f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dataset = InferenceDataset(texts=list(contents_df.content_text.values), tokenizer_name='sentence-transformers/all-MiniLM-L6-v2', max_len=128)\n",
    "content_dataloader = DataLoader(content_dataset, num_workers=16, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# # \n",
    "# del contents_df[\"text\"]\n",
    "# del contents_df\n",
    "\n",
    "# import gc\n",
    "# gc.collect()\n",
    "\n",
    "content_embs = []\n",
    "\n",
    "for inputs in tqdm(content_dataloader):\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k].to(device)\n",
    "    out = model.feature(inputs)\n",
    "    content_embs.extend(out.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a621519-5667-4375-bfaf-4212315d6330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load from saved files\n",
    "# torch.save(topic_embs, \"./data/topic_embs.pt\")\n",
    "# torch.save(content_embs, \"./data/content_embs.pt\")\n",
    "\n",
    "# # # topic_embs = torch.load(\"./data/topic_embs.pt\")\n",
    "# # # content_embs = torch.load(\"./data/content_embs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7391a5-2bcb-4487-978a-532d6111c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release memory\n",
    "import gc\n",
    "\n",
    "# del model\n",
    "del state_dict\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fuzzywuzzy annoy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1263a349-1632-4ff7-81dc-3b3023767742",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda876a8-cfb2-4697-88d7-e313f3a1669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics = topics_df[topics_df.has_content==True][['id', 'title', 'language']].reset_index(drop=True)\n",
    "\n",
    "topics = topics_df\n",
    "\n",
    "test = topics\n",
    "all_content_ids = contents_df.id.to_numpy()\n",
    "all_content_titles = contents_df.title.to_numpy()\n",
    "all_content_language = contents_df.language.to_numpy()\n",
    "all_test_ids = list(topics.id)\n",
    "all_test_title = list(topics.title)\n",
    "all_test_language = list(test.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e8833-46a1-494a-a09b-5300a26dde3e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install cupy-cuda11x\n",
    "# !pip install cuml-cu11 --extra-index-url=https://pypi.ngc.nvidia.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9a4a87-2cc8-459d-abe5-6a8162bda6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "from cuml.metrics import pairwise_distances\n",
    "from cuml.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c56c8-ed96-4e34-9c1e-9ecba6be0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer predictions to gpu\n",
    "topic_embs_gpu = cp.array(topic_embs)\n",
    "content_embs_gpu = cp.array(content_embs)\n",
    "\n",
    "# Release memory\n",
    "torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "\n",
    "# KNN model\n",
    "print(' ')\n",
    "print('Training KNN model...')\n",
    "neighbors_model = NearestNeighbors(n_neighbors = 50, metric = 'cosine')\n",
    "neighbors_model.fit(content_embs_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c11f0-55b8-49ea-ab28-6d77f8dacb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = neighbors_model.kneighbors(topic_embs_gpu, return_distance = True)\n",
    "predictions = []\n",
    "for k in tqdm(range(len(indices))):\n",
    "    pred = indices[k]\n",
    "    p = ' '.join([contents_df.loc[ind, 'id'] for ind in pred.get()])\n",
    "    predictions.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_topics = []\n",
    "pair_contents = []\n",
    "pair_distances = []\n",
    "\n",
    "for k in tqdm(range(len(indices))):\n",
    "    pred = indices[k].get()\n",
    "    dis = distances[k].get()\n",
    "\n",
    "    for i in range(len(pred)):\n",
    "        ind = pred[i]\n",
    "        pair_topics.append(all_test_ids[k])\n",
    "        pair_contents.append(contents_df.loc[ind, 'id'])\n",
    "        pair_distances.append(dis[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf83c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_preds = pd.DataFrame({\n",
    "    \"topic_id\": pair_topics,\n",
    "    \"content_ids\": pair_contents,\n",
    "    \"distance\": pair_distances\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd92aa-6dfd-4051-97fd-2dcf1a7e5c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_pos_score(y_true, y_pred, top_k):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()[:top_k]))\n",
    "    int_true = np.array([len(x[0] & x[1]) / len(x[0]) for x in zip(y_true, y_pred)])\n",
    "    return round(np.mean(int_true), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63568d74-148b-4a6b-9084-cf6a62a70504",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_preds = pd.DataFrame({\n",
    "    'topic_id': all_test_ids,\n",
    "    'content_ids': predictions # [\" \".join(p) for p in predictions]\n",
    "}).sort_values(\"topic_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae394d9-0f18-4fb1-8f0e-7fadf99cca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = corrs_df[corrs_df.topic_id.isin(val_topic_ids)].sort_values(\"topic_id\")    \n",
    "for k in [5, 10, 20, 50, 100, 200, 500, 1000, 1500, 2000]:\n",
    "    print(\"top_k =\", k, \"max_score =\", get_pos_score(gt[\"content_ids\"], knn_preds.sort_values(\"topic_id\")[\"content_ids\"], k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4d203-a366-4ce4-bc9f-8fb4f1ab5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = knn_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c7cac-b779-436b-92ec-ac7e6129678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b834acf9-92c7-44d8-a9cf-d0e1cc42654b",
   "metadata": {},
   "source": [
    "#### get neighbors using annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7e8f3-67a8-4d9a-8f89-e3d1e9e0894e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# from annoy import AnnoyIndex\n",
    "\n",
    "\n",
    "# content_forest = AnnoyIndex(content_embs[0].shape[0], metric='angular')\n",
    "# for i, item in tqdm(enumerate(content_embs), total=len(content_embs)):\n",
    "#     content_forest.add_item(i, item)\n",
    "# content_forest.build(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b24495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes_dict = {}\n",
    "# fuzzy_dict = {}\n",
    "# classification_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4596424-e963-48fd-8f4a-627e77f82c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: find by distance instead\n",
    "\n",
    "# nearest_content_count = 2000\n",
    "# fuzzy_filter = 80\n",
    "# THRESHOLD = 0\n",
    "# # for fuzzy_filter in range(5, 50, 5):\n",
    "# #     for t in range(1, 10, 2):\n",
    "# #         THRESHOLD = t / 100\n",
    "# preds = []\n",
    "# for i, t_e in tqdm(enumerate(topic_embs), total=len(topic_embs), desc=f'Getting Preds'):\n",
    "#     if i in indexes_dict:\n",
    "#         indexes, distances = indexes_dict[i]\n",
    "#     else:\n",
    "#         indexes, distances = content_forest.get_nns_by_vector(\n",
    "#             # F.normalize(torch.from_numpy(t_e), p=2, dim=0),\n",
    "#             t_e,\n",
    "#             nearest_content_count,\n",
    "#             include_distances=True\n",
    "#         )\n",
    "#         # indexes = [i for i, d in zip(indexes, distances) if d < 10]\n",
    "#         indexes_dict[i] = indexes, distances\n",
    "\n",
    "#     topic_id = all_test_ids[i]\n",
    "#     topic_text = all_test_title[i]\n",
    "#     topic_lang = all_test_language[i]\n",
    "\n",
    "#     # for idx in indexes:\n",
    "#     #     if topic_lang != all_content_language[idx]:\n",
    "#     #         indexes.remove(idx)\n",
    "    \n",
    "#     # filtered_indexes = []\n",
    "#     # for idx in indexes:\n",
    "#     #     if topic_lang != all_content_language[idx]:\n",
    "#     #         continue\n",
    "#     #     if (i, idx) in fuzzy_dict:\n",
    "#     #         fuzzy_value = fuzzy_dict[(i, idx)]\n",
    "#     #     else:\n",
    "#     #         fuzzy_value = fuzz.token_set_ratio(all_content_titles[idx], topic_text)\n",
    "#     #         fuzzy_dict[(i, idx)] = fuzzy_value\n",
    "        \n",
    "#     #     if fuzzy_value > fuzzy_filter:\n",
    "#     #         filtered_indexes.append(idx)\n",
    "        \n",
    "#     #     if (i, idx) in classification_dict:\n",
    "#     #         score = classification_dict[(i, idx)]\n",
    "#     #     else:\n",
    "#     #         topic_features = torch.from_numpy(t_e).to(device)\n",
    "#     #         content_features = torch.from_numpy(content_embs[idx]).to(device)\n",
    "#     #         score = torch.sigmoid(model.fc(torch.cat([topic_features, content_features, topic_features - content_features], -1))).item()\n",
    "#     #         classification_dict[(i, idx)] = score\n",
    "#     #     if score < THRESHOLD and idx in filtered_indexes:\n",
    "#     #         filtered_indexes.remove(idx)\n",
    "#     # ind2dis = {ind: d for ind, d in zip(indexes, distances)}\n",
    "#     # if len(filtered_indexes) == 0:\n",
    "#     #     indexes = filtered_indexes[:8] # list(set(filtered_indexes + indexes[:8-len(filtered_indexes)]))\n",
    "#     # else:\n",
    "#     #     indexes = filtered_indexes[:8]\n",
    "#     content_ids = all_content_ids[indexes]\n",
    "#     preds.append({\n",
    "#         'topic_id': topic_id,\n",
    "#         'content_ids': ' '.join(content_ids),\n",
    "#         # 'distances': ' '.join([str(ind2dis[ind]) for ind in indexes]),\n",
    "#     })\n",
    "# preds = pd.DataFrame.from_records(preds)\n",
    "\n",
    "# preds.to_csv('submission.csv', index=False)\n",
    "\n",
    "# if not TEST_MODE:\n",
    "#     from engine import f2_score\n",
    "#     gt = corrs_df[corrs_df.topic_id.isin(val_topic_ids)].sort_values(\"topic_id\")    \n",
    "#     preds = preds.sort_values(\"topic_id\")    \n",
    "#     print(\"fuzzy_filter\", fuzzy_filter, \"THRESHOLD:\", THRESHOLD, \"f2_score\", f2_score(gt[\"content_ids\"], preds[\"content_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2466ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in [10, 20, 50, 100, 200, 500, 1000, 1500, 2000]:\n",
    "#     print(\"top_k =\", k, \"max_score =\", get_pos_score(gt[\"content_ids\"], preds[\"content_ids\"], k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874af05f-4aeb-4cf3-b46e-1f691ea9c474",
   "metadata": {},
   "source": [
    "### filter by using cross-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56446614-1b16-4b5e-9e0f-deb336f7307f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cross_encoder_model = Model(\n",
    "#     tokenizer_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "#     model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "#     objective=\"classification\",\n",
    "#     is_sentence_transformers=True\n",
    "# )\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# cross_encoder_model = cross_encoder_model.to(device)\n",
    "\n",
    "# weights_path = \"./data/classification_model.bin\"\n",
    "\n",
    "# state_dict = torch.load(weights_path)\n",
    "# cross_encoder_model.load_state_dict(state_dict)\n",
    "# cross_encoder_model.eval()\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c9480-5f67-402e-b106-d66869744fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataset import init_tokenizer\n",
    "\n",
    "# class CrossEncoderDataset(Dataset):\n",
    "#     def __init__(self, df, tokenizer_name='sentence-transformers/all-MiniLM-L6-v2', max_len=128):\n",
    "#         self.df = df\n",
    "#         self.topic_texts = []\n",
    "#         self.content_texts = []\n",
    "#         for i, row in tqdm(df.iterrows()):\n",
    "#             if row[\"content_ids\"]:\n",
    "#                 for content_id in row[\"content_ids\"].split(\" \"):\n",
    "#                     self.topic_texts.append(topic_dict[row[\"topic_id\"]])\n",
    "#                     self.content_texts.append(content_dict[content_id])\n",
    "                    \n",
    "#         self.tokenizer = init_tokenizer(tokenizer_name)\n",
    "#         self.max_len = max_len\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.topic_texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         topic_text = self.topic_texts[idx]\n",
    "#         content_text = self.content_texts[idx]\n",
    "        \n",
    "#         # topic\n",
    "#         topic_inputs = self.tokenizer.encode_plus(\n",
    "#             topic_text, \n",
    "#             return_tensors = None, \n",
    "#             add_special_tokens = True, \n",
    "#             max_length = self.max_len,\n",
    "#             padding='max_length',\n",
    "#             truncation = True\n",
    "#         )\n",
    "#         for k, v in topic_inputs.items():\n",
    "#             topic_inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "            \n",
    "#         # content\n",
    "#         content_inputs = self.tokenizer.encode_plus(\n",
    "#             content_text, \n",
    "#             return_tensors = None, \n",
    "#             add_special_tokens = True, \n",
    "#             max_length = self.max_len,\n",
    "#             padding='max_length',\n",
    "#             truncation = True\n",
    "#         )\n",
    "#         for k, v in content_inputs.items():\n",
    "#             content_inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "\n",
    "#         combined_inputs = self.tokenizer.encode_plus(\n",
    "#             topic_text,\n",
    "#             content_text,\n",
    "#             return_tensors = None, \n",
    "#             add_special_tokens = True, \n",
    "#             max_length = self.max_len,\n",
    "#             padding='max_length',\n",
    "#             truncation = True\n",
    "#         )\n",
    "#         for k, v in combined_inputs.items():\n",
    "#             combined_inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "            \n",
    "#         return topic_inputs, content_inputs, combined_inputs, 0\n",
    "\n",
    "\n",
    "# def cross_encoder_collate_fn(batch):\n",
    "#     batch = default_collate(batch)\n",
    "\n",
    "#     topic_inputs, content_inputs, combined_inputs, labels = batch\n",
    "#     mask_len = int(topic_inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "#     for k, v in topic_inputs.items():\n",
    "#         topic_inputs[k] = topic_inputs[k][:,:mask_len]\n",
    "\n",
    "#     mask_len = int(content_inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "#     for k, v in content_inputs.items():\n",
    "#         content_inputs[k] = content_inputs[k][:,:mask_len]\n",
    "\n",
    "#     mask_len = int(combined_inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "#     for k, v in combined_inputs.items():\n",
    "#         combined_inputs[k] = combined_inputs[k][:,:mask_len]\n",
    "\n",
    "\n",
    "#     return {\n",
    "#         \"topic_inputs\": topic_inputs,\n",
    "#         \"content_inputs\": content_inputs,\n",
    "#         \"combined_inputs\": combined_inputs,\n",
    "#         \"labels\": labels\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b7252-5328-48b6-820c-5c5cfac1ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db48cf09-b488-4939-87ec-e19a04a7471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df = data_df[data_df[\"fold\"] == fold]\n",
    "# val_df[\"topic_id\"] = val_df[\"topics_ids\"]\n",
    "# val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99177dac-feab-42ba-8b78-6bae5195c30f",
   "metadata": {},
   "source": [
    "##### re-calucate on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa21e6-b151-421a-a0f4-8ecb8fe8d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score, accuracy_score, recall_score\n",
    "\n",
    "# ceds = CrossEncoderDataset(val_df, tokenizer_name=\"sentence-transformers/all-MiniLM-L6-v2\", max_len=256)\n",
    "\n",
    "# ce_dataloader = DataLoader(ceds, batch_size=64, num_workers=16, shuffle=False, collate_fn=cross_encoder_collate_fn)\n",
    "\n",
    "# res = []\n",
    "\n",
    "# for inputs in tqdm(ce_dataloader):\n",
    "#     for k, v in inputs.items():\n",
    "#         inputs[k] = inputs[k].to(device)\n",
    "#     out = cross_encoder_model(**inputs)\n",
    "#     out = torch.sigmoid(out)\n",
    "#     res.extend(out.cpu().detach().numpy())\n",
    "#     # break\n",
    "\n",
    "# f1_score(list(val_df.target.values), [int(r[0] > 0.5) for r in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100acf18-70c5-4157-b89f-75cedcbb9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = val_df.groupby(\"topic_id\", group_keys=False).agg({\"content_ids\": \" \".join})\n",
    "# new_df[\"topic_id\"] = new_df.index\n",
    "# new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11564a94-e09c-4b77-b996-704327f73936",
   "metadata": {},
   "source": [
    "##### calculate on predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc30573-6a23-46f0-9669-c3ad16fafb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ceds = CrossEncoderDataset(pair_preds, tokenizer_name=\"sentence-transformers/all-MiniLM-L6-v2\", max_len=256)\n",
    "\n",
    "# ce_dataloader = DataLoader(ceds, batch_size=32, num_workers=64, shuffle=False, collate_fn=cross_encoder_collate_fn)\n",
    "\n",
    "# res = []\n",
    "\n",
    "# for inputs in tqdm(ce_dataloader):\n",
    "#     for k, v in inputs.items():\n",
    "#         inputs[k] = inputs[k].to(device)\n",
    "#     out = cross_encoder_model(**inputs)\n",
    "#     out = torch.sigmoid(out)\n",
    "#     res.extend(out.cpu().detach().numpy())\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ad058-4a82-4cef-b9fb-abc66b84651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_topics = []\n",
    "# pred_contents = []\n",
    "\n",
    "# for i, row in tqdm(preds.iterrows()):\n",
    "#     if row[\"content_ids\"]:\n",
    "#         for content_id in row[\"content_ids\"].split(\" \"):\n",
    "#             pred_topics.append(row[\"topic_id\"])\n",
    "#             pred_contents.append(content_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6a4fd5-7f63-43ce-91cd-a1af2a12a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for topic_id, content_id, score in zip(pred_topics, pred_contents, res):\n",
    "# new_pred_df = pd.DataFrame({\n",
    "#     \"topic_id\": pred_topics,\n",
    "#     \"content_id\": pred_contents,\n",
    "#     \"score\": [r[0] for r in res]\n",
    "# })\n",
    "\n",
    "# # new_pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8152cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair_preds[\"id\"] = (pair_preds[\"topic_id\"] + pair_preds[\"content_ids\"]).astype(str) \n",
    "# new_pred_df[\"id\"] = (new_pred_df[\"topic_id\"] + new_pred_df[\"content_id\"]).astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e14a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfed9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ad067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_pred_df = new_pred_df.sort_values(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair_preds = pair_preds.sort_values(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16224bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_preds = pair_preds.merge(new_pred_df, on=\"id\")\n",
    "# merge_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # original 0.005, 3\n",
    "# merge_preds[\"ensemble_score\"] = merge_preds[\"score\"]**0.005 * (1 - merge_preds[\"distance\"])**3\n",
    "# merge_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de7e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_preds[\"topic_id\"] = merge_preds[\"topic_id_x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23db1b85-ce42-41c8-990a-cd8bd53c8912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_id_to_language = {}\n",
    "# for i, row in tqdm(topic_df.iterrows()):\n",
    "#     topic_id_to_language[row[\"id\"]] = row[\"language\"]\n",
    "\n",
    "# content_id_to_language = {}\n",
    "# for i, row in tqdm(content_df.iterrows()):\n",
    "#     content_id_to_language[row[\"id\"]] = row[\"language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df21178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_indices = []\n",
    "\n",
    "# # for i, row in tqdm(merge_preds.iterrows()):\n",
    "# #     if topic_id_to_language[row[\"topic_id\"]] != content_id_to_language[row[\"content_id\"]]:\n",
    "# #         drop_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cfe068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(drop_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05388962-0892-40a0-a78a-fd2f4b4aefaf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# outputs = merge_preds.drop(index=drop_indices)\n",
    "# from utils import f2_score\n",
    "\n",
    "# for i in range(260, 280):\n",
    "#     threshold = i / 1000\n",
    "#     thresholded_outputs = outputs[(outputs[\"ensemble_score\"] >= threshold)].groupby('topic_id').agg({'content_id': \" \".join})\n",
    "#     thresholded_outputs[\"topic_id\"] = thresholded_outputs.index\n",
    "#     # TODO: we need to merge with those topics doesn't have any contents\n",
    "#     no_content_topics = list(set(new_pred_df.topic_id.values).difference(set(thresholded_outputs.topic_id.values)))\n",
    "\n",
    "#     no_content_topics_contents = []\n",
    "#     for topic_id in no_content_topics:\n",
    "#         top_contents = outputs[outputs.topic_id == topic_id].sort_values(\"ensemble_score\").content_ids.values[-5:]\n",
    "#         # top_contents = merge_preds[merge_preds.topic_id == topic_id].sort_values(\"score\").content_ids.values[-5:]\n",
    "#         # top_contents = merge_preds[merge_preds.topic_id == topic_id].sort_values(\"distance\").content_ids.values[:5]\n",
    "#         no_content_topics_contents.append(\" \".join(top_contents))\n",
    "        \n",
    "        \n",
    "#         # no_content_topics_contents.append(\" \")\n",
    "\n",
    "#     no_content_df = pd.DataFrame({\n",
    "#         \"topic_id\": no_content_topics,\n",
    "#         \"content_id\": no_content_topics_contents\n",
    "#     })\n",
    "\n",
    "#     final_predictions = pd.concat([thresholded_outputs, no_content_df])\n",
    "#     final_predictions.sort_values(\"topic_id\")\n",
    "\n",
    "#     gt = corrs_df[corrs_df.topic_id.isin(val_topic_ids)].sort_values(\"topic_id\")    \n",
    "#     thresholded_score = f2_score(gt[\"content_ids\"], final_predictions.sort_values(\"topic_id\")[\"content_id\"])\n",
    "#     print(\"threshold =\", threshold, \"f2_score =\", thresholded_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdbccfb-a1ea-4818-8bff-2451b020ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_final_predictions = final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd83cb6-a604-49cb-b4c9-b4964f23c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_predictions[final_predictions.topic_id.isin(no_content_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e3e8ed-033a-4a68-b2a3-9420edd159dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt[gt.topic_id.isin(no_content_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3c837-aa8d-4c38-98ea-3254f691ad19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19cfa6cd-df89-46f1-9c55-b9a726c53b39",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704086b8-9f1c-436d-822c-c37d758e2373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6376eb-7c62-492f-b604-d3b3b5cfbb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_topic_ids = []\n",
    "# gt_content_ids = []\n",
    "# gt_merge_ids = []\n",
    "\n",
    "# for i, row in gt.iterrows():\n",
    "#     topic_id = row.topic_id\n",
    "#     for content_id in row.content_ids.split(\" \"):\n",
    "#         gt_topic_ids.append(topic_id)\n",
    "#         gt_content_ids.append(content_id)\n",
    "#         gt_merge_ids.append(topic_id + content_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e8f98-322e-4dad-8c6c-c2f315d345a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_pair_df = pd.DataFrame({\n",
    "#     \"topic_id\": gt_topic_ids,\n",
    "#     \"content_id\": gt_content_ids,\n",
    "#     \"id\": gt_merge_ids\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e0625-dff8-45d1-a204-86293b124144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_preds[\"target\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c434f036-632d-487c-a70e-aab811dfa4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge_preds[][\"target\"] = 1\n",
    "# merge_preds.loc[merge_preds.id.isin(gt_pair_df.id.values), \"target\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ffa66-94b2-446b-a5ce-a6e14bff740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(merge_preds.target.values, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c4b637-6618-488c-9787-e27ce512b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854ccdf-b7dc-44eb-9fcd-fcf97a810652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a9ad6-c8d4-475c-a26b-2fb8661ab2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb65eef-0223-4281-be39-5db5643c968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate training and validation set for xgbooost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0994f-0033-4512-be56-38a9db240dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"./data/new_train_supervised_df.csv\")\n",
    "# val_df = pd.read_csv(\"./data/new_val_supervised_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f186ad1-87db-4fda-877a-7bc42117772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d659d-cd0c-4ac1-a646-d32cda7ff9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dataset = InferenceDataset(texts=list(topic_df.topic_text.values), tokenizer_name='sentence-transformers/all-MiniLM-L6-v2', max_len=128)\n",
    "topic_dataloader = DataLoader(topic_dataset, num_workers=16, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce592879-e038-4416-a82a-8fa19cb77dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "all_topic_embs = []\n",
    "\n",
    "for inputs in tqdm(topic_dataloader):\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k].to(device)\n",
    "    out = model.feature(inputs)\n",
    "    all_topic_embs.extend(out.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b8ca4-c266-4a05-8e0c-9bf49f59e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id_to_emb = {}\n",
    "for id, emb in zip(topic_df.id.values, all_topic_embs):\n",
    "    topic_id_to_emb[id] = emb\n",
    "\n",
    "content_id_to_emb = {}\n",
    "for id, emb in zip(contents_df.id.values, content_embs):\n",
    "    content_id_to_emb[id] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9013dcf0-c061-454b-92aa-9f36dc52665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# train_distances = []\n",
    "# for i, row in tqdm(train_df.iterrows()):\n",
    "#     a = topic_id_to_emb[row.topics_ids]\n",
    "#     b = content_id_to_emb[row.content_ids]\n",
    "#     train_distances.append(\n",
    "#         1 - dot(a, b)/(norm(a)*norm(b))\n",
    "#     )\n",
    "# train_df[\"distance\"] = train_distances\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b513f2-0f04-4a52-8690-df22289ced1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = set()\n",
    "\n",
    "# for i, row in corrs_df[corrs_df.topic_id.isin(preds.topic_id.values)].iterrows():\n",
    "#     topic_id = row.topic_id\n",
    "#     for content_id in row.content_ids.split(\" \"):\n",
    "#         pairs.add((topic_id, content_id, 1))\n",
    "        \n",
    "# for i, row in preds.iterrows():\n",
    "#     topic_id = row.topic_id\n",
    "#     for content_id in row.content_ids.split(\" \"):\n",
    "#         if (topic_id, content_id, 1) in pairs:\n",
    "#             continue\n",
    "        \n",
    "#         pairs.add((topic_id, content_id, 0))\n",
    "\n",
    "\n",
    "for i, row in preds.iterrows():\n",
    "    topic_id = row.topic_id\n",
    "    for content_id in row.content_ids.split(\" \"):\n",
    "        pairs.add((topic_id, content_id, 0))\n",
    "\n",
    "for i, row in corrs_df[corrs_df.topic_id.isin(preds.topic_id.values)].iterrows():\n",
    "    topic_id = row.topic_id\n",
    "    for content_id in row.content_ids.split(\" \"):\n",
    "        if (topic_id, content_id, 0) in pairs:\n",
    "            pairs.remove((topic_id, content_id, 0))\n",
    "            pairs.add((topic_id, content_id, 1))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a75e8-3780-46f6-af4a-494d55ad5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ids = [pair[0] for pair in pairs]\n",
    "content_ids = [pair[1] for pair in pairs]\n",
    "targets = [pair[2] for pair in pairs]\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    \"topics_ids\": topic_ids,\n",
    "    \"content_ids\": content_ids,\n",
    "    \"target\": targets\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a63a5-b912-4e36-a335-3ae5e15a832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_distances = []\n",
    "for i, row in tqdm(val_df.iterrows()):\n",
    "    a = topic_id_to_emb[row.topics_ids]\n",
    "    b = content_id_to_emb[row.content_ids]\n",
    "    val_distances.append(\n",
    "        1 - dot(a, b)/(norm(a)*norm(b))\n",
    "    )\n",
    "    \n",
    "val_df[\"distance\"] = val_distances\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e27f28-362b-4a02-8f23-35c89144e441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a04514-ec27-46ac-8ed8-09e96beb9fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = []\n",
    "# for i, row in tqdm(train_df.iterrows()):\n",
    "#     feature = np.concatenate([\n",
    "#         topic_id_to_emb[row.topics_ids],\n",
    "#         content_id_to_emb[row.content_ids],\n",
    "#         # [row.score],\n",
    "#         [row.distance]\n",
    "#     ])\n",
    "    \n",
    "#     x_train.append(feature)\n",
    "    \n",
    "# y_train = train_df.target.values.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a7557f-679e-4bb8-b7c9-6294dc617121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_valid = []\n",
    "# for i, row in tqdm(val_df.iterrows()):\n",
    "#     feature = np.concatenate([\n",
    "#         topic_id_to_emb[row.topics_ids],\n",
    "#         content_id_to_emb[row.content_ids],\n",
    "#         # [row.score],\n",
    "#         [row.distance]\n",
    "#     ])\n",
    "    \n",
    "#     x_valid.append(feature)\n",
    "    \n",
    "# y_valid = val_df.target.values.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be7677-a532-414d-bdaf-f64ae15f4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def unit_vector(vector):\n",
    "#     \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "#     return vector / np.linalg.norm(vector)\n",
    "\n",
    "# def angle_between(v1, v2):\n",
    "#     \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "#             >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "#             1.5707963267948966\n",
    "#             >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "#             0.0\n",
    "#             >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "#             3.141592653589793\n",
    "#     \"\"\"\n",
    "#     v1_u = unit_vector(v1)\n",
    "#     v2_u = unit_vector(v2)\n",
    "#     return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "# def euclidian_distance(a, b):\n",
    "#     return np.linalg.norm(a-b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e100356-04a8-446b-bf44-17ed7f79370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i, row in tqdm(val_df.iterrows()):\n",
    "    feature = np.concatenate([\n",
    "        topic_id_to_emb[row.topics_ids],\n",
    "        content_id_to_emb[row.content_ids],\n",
    "        # [angle_between(topic_id_to_emb[row.topics_ids], content_id_to_emb[row.content_ids])],\n",
    "        [row.distance]\n",
    "    ])\n",
    "    \n",
    "    X.append(feature)\n",
    "y = val_df.target.values.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6376e-68fe-4ebb-a6f7-c1450647b888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ff8c0-09c1-4431-be39-4f139450fe8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = []\n",
    "# for i, row in tqdm(merge_preds.iterrows()):\n",
    "#     feature = np.concatenate([\n",
    "#         topic_id_to_emb[row.topic_id],\n",
    "#         content_id_to_emb[row.content_id],\n",
    "#         # [row.score],\n",
    "#         [angle_between(topic_id_to_emb[row.topic_id], content_id_to_emb[row.content_id])],\n",
    "#         # [euclidian_distance(topic_id_to_emb[row.topic_id], content_id_to_emb[row.content_id])],\n",
    "#         # [manhattan(topic_id_to_emb[row.topic_id], content_id_to_emb[row.content_id])],\n",
    "#         [row.distance]\n",
    "#     ])\n",
    "    \n",
    "#     X.append(feature)\n",
    "\n",
    "# # X = np.stack([merge_preds.distance.values, merge_preds.score.values]).transpose()\n",
    "# y = merge_preds.target.values.astype(np.uint8)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200762f-c81b-49ef-8f38-c46431096a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_train = np.concatenate([\n",
    "    x_train,\n",
    "    np.concatenate([x_train[:, :384], x_train[:, 384:768], x_train[:, 768:]], 1)\n",
    "], 0)\n",
    "y_train = np.concatenate([y_train, y_train], 0)\n",
    "\n",
    "# x_valid = np.array(x_valid)\n",
    "# x_valid = np.concatenate([\n",
    "#     x_valid,\n",
    "#     np.concatenate([x_valid[:, :384], x_valid[:, 384:768], x_valid[:, 768:]], 1)\n",
    "# ], 0)\n",
    "# y_valid = np.concatenate([y_valid, y_valid], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6a886-b3f2-4743-857d-f2ab5908c57c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "def xgb_metric(preds, dmatrix):\n",
    "    return \"f1\", f1_score(dmatrix.get_label(), preds >= 0.5)\n",
    "\n",
    "\n",
    "xgb_cfg = {\n",
    "        \"n_estimators\": 2000,\n",
    "        \"learning_rate\": 1e-2,\n",
    "        # \"subsample\": 0.6,\n",
    "        # \"colsample_bytree\": 0.8,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"nthread\": os.cpu_count(),\n",
    "        # \"scale_pos_weight\": (feats_df[\"cancer\"] == 0).sum() / (feats_df[\"cancer\"] == 1).sum(),\n",
    "    }\n",
    "fit_params = {\n",
    "        \"verbose\": True,\n",
    "        \"eval_metric\": xgb_metric,\n",
    "    }\n",
    "\n",
    "xgb = XGBClassifier(**xgb_cfg)\n",
    "\n",
    "xgb.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], **fit_params)\n",
    "\n",
    "ckpt_path = \"xbg_augment_2000.pth\"\n",
    "xgb.save_model(ckpt_path)\n",
    "fold_preds = xgb.predict_proba(x_valid, ntree_limit=xgb.best_ntree_limit)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85361168-d7c0-4218-b03d-16d0274954bd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb.load_model(ckpt_path)\n",
    "# xgb.predict_proba(X, ntree_limit=xgb.best_ntree_limit)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d12a7-501e-47bf-9dd8-56ef9dc361f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[\"topic_id\"] =val_df[\"topics_ids\"]  \n",
    "val_df[\"ensemble_score\"] = xgb.predict_proba(X, ntree_limit=xgb.best_ntree_limit)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e720cbdd-bbcf-4b44-83d5-ebd6364bbcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.predict_proba(X, ntree_limit=xgb.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5356c-ce12-48b2-b20c-59b4480e5fe2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# outputs = merge_preds.drop(index=drop_indices)\n",
    "from utils import f2_score\n",
    "\n",
    "for i in range(110, 130):\n",
    "    threshold = i / 1000\n",
    "    thresholded_outputs = val_df[(val_df[\"ensemble_score\"] >= threshold)].groupby('topic_id').agg({'content_ids': \" \".join})\n",
    "    thresholded_outputs[\"topic_id\"] = thresholded_outputs.index\n",
    "    # TODO: we need to merge with those topics doesn't have any contents\n",
    "    no_content_topics = list(set(val_df.topic_id.values).difference(set(thresholded_outputs.topic_id.values)))\n",
    "\n",
    "    no_content_topics_contents = []\n",
    "    for topic_id in no_content_topics:\n",
    "        top_contents = val_df[val_df.topic_id == topic_id].sort_values(\"ensemble_score\").content_ids.values[-5:]\n",
    "        # top_contents = merge_preds[merge_preds.topic_id == topic_id].sort_values(\"score\").content_ids.values[-5:]\n",
    "        # top_contents = merge_preds[merge_preds.topic_id == topic_id].sort_values(\"distance\").content_ids.values[:5]\n",
    "        no_content_topics_contents.append(\" \".join(top_contents))\n",
    "        \n",
    "        # no_content_topics_contents.append(\" \")\n",
    "\n",
    "    no_content_df = pd.DataFrame({\n",
    "        \"topic_id\": no_content_topics,\n",
    "        \"content_ids\": no_content_topics_contents\n",
    "    })\n",
    "\n",
    "    final_predictions = pd.concat([thresholded_outputs, no_content_df])\n",
    "    final_predictions.sort_values(\"topic_id\")\n",
    "\n",
    "    gt = corrs_df[corrs_df.topic_id.isin(val_topic_ids)].sort_values(\"topic_id\")    \n",
    "    thresholded_score = f2_score(gt[\"content_ids\"], final_predictions.sort_values(\"topic_id\")[\"content_ids\"])\n",
    "    print(\"threshold =\", threshold, \"f2_score =\", thresholded_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d436d4a-f524-4127-8970-2c97debdd7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f0660-355a-457e-b2ba-47307f315572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3661d16-4cf4-4281-b0c6-e83acb27c350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc85beda-ec87-4ed1-8e58-9c33245d4ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e07aa-9810-4079-9084-a0f1d7514610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52335c-f6f2-432d-b700-d168cd5d84ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d6dea2-140c-434c-9420-c593488bf140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e2b96-d28e-4e36-9fd2-acba3dcd4d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ee93e-80d8-4250-a406-94ae99495521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a8a288-1ddb-489f-a5fc-2c882a26962c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf3adf-9ca7-408d-8953-7cd88e61ff2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b4f55e1-d483-471a-ba79-8b24a75410b7",
   "metadata": {},
   "source": [
    "#### Try ensemble classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad69836-6de5-42ef-9ad5-09d20c67f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_dict = {}\n",
    "for i, row in cls_final_predictions.iterrows():\n",
    "    cls_dict[row.topic_id] = row.content_id\n",
    "\n",
    "    \n",
    "xgb_dict = {}\n",
    "for i, row in final_predictions.iterrows():\n",
    "    xgb_dict[row.topic_id] = row.content_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258a935-83c6-452b-a0f8-ed37537cf64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_ids_1 = []\n",
    "content_ids_2 = []\n",
    "topic_ids = []\n",
    "\n",
    "for k, v in cls_dict.items():\n",
    "    topic_ids.append(k)\n",
    "    content_ids_1.append(v)\n",
    "    content_ids_2.append(xgb_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b963b93-afe2-4a5e-87f0-8949f8e63ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.DataFrame({\n",
    "    \"topic_id\": topic_ids,\n",
    "    \"content_ids_1\": content_ids_1,\n",
    "    \"content_ids_2\": content_ids_2,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e4c52-a054-4f58-9c6a-b81b43995439",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb32e44-267e-466c-b480-9f8d8fcc0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_content_ids = []\n",
    "\n",
    "for i, row in merged.iterrows():\n",
    "    new_ids = list(set(row.content_ids_1.split(\" \")).union(set(row.content_ids_2.split(\" \"))))\n",
    "    if len(new_ids) == 0:\n",
    "        new_ids = row.content_ids_2.split(\" \")\n",
    "    new_content_ids.append(\n",
    "        \" \".join(new_ids)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a585c-c569-4aa5-bfaa-8db66954f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[\"new_content_ids\"] = new_content_ids\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ccbb15-c846-4235-b5f4-1776640af05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_score(gt[\"content_ids\"], merged.sort_values(\"topic_id\")[\"new_content_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44b2f4-cf1a-49b4-9598-d382d1ec44b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b07bd3-4cda-4d0e-829c-68908ff48023",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.sort_values(\"topic_id\")[\"new_content_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ac771-db62-469e-afe1-604bfd2a9d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_score(gt[\"content_ids\"], final_predictions.sort_values(\"topic_id\")[\"content_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b70ac2-201d-4b45-83e7-aa154a95bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_score(gt[\"content_ids\"], cls_final_predictions.sort_values(\"topic_id\")[\"content_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f2e9af-ee4a-4944-b4d6-10f5b95c48f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02de97-58da-45ba-a7ff-d8af9d532001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eafc4ae-db49-4949-8d74-2b31976758d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f52c6b9-2158-43a7-8bd0-36862ea50430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e924dbe9-1da1-4002-b71c-bac663cdb777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b2f837-f19f-4b20-ab25-f264c3cd7fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f31c6b-a0d6-42b6-92c7-537b43a76e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c73121-d45f-4e79-b0a5-352d679ba700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd63ffc-2c80-4fc1-9312-4d4e59cdfd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = final_predictions.sort_values(\"topic_id\")\n",
    "\n",
    "for index in range(11, len(gt)):\n",
    "    topic_id = topic_df[topic_df.id == gt.topic_id.values[index]].id.values[0]\n",
    "    row_predictions = final_predictions[\"content_id\"][index: index + 1].values[0].split(\" \")\n",
    "    row_gt = gt[\"content_ids\"][index: index + 1].values[0].split(\" \")\n",
    "    \n",
    "    print(\"topic_id\", topic_id)\n",
    "    print(\"predictions:\", row_predictions)\n",
    "    print(\"gt:\", row_gt)\n",
    "\n",
    "    tp = set(row_predictions).intersection(set(row_gt))\n",
    "    fp = set(row_predictions).difference(set(row_gt))\n",
    "    fn = set(row_gt).difference(set(row_predictions))\n",
    "\n",
    "    print(\"true positive:\", tp)\n",
    "    print(\"false positive:\", fp)\n",
    "    print(\"false negative:\", fn)\n",
    "\n",
    "    print(f2_score(gt[\"content_ids\"][index:index + 1], final_predictions[\"content_id\"][index: index + 1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a2e50-29a9-415c-9d94-5c7f7ecd40c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ALL\")\n",
    "merge_preds[(merge_preds.topic_id == topic_id) & (merge_preds.content_id.isin(row_predictions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac0eb2c-3ce3-4eb5-acda-6f854d40ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True Positive\")\n",
    "merge_preds[(merge_preds.topic_id == topic_id) & (merge_preds.content_id.isin(tp))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c040d-952e-4923-8945-1cb6764ac0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"False Negative\")\n",
    "merge_preds[(merge_preds.topic_id == topic_id) & (merge_preds.content_id.isin(fn))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aced63-12de-4b7f-84ea-8a82bfceb0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"False Positive\")\n",
    "merge_preds[(merge_preds.topic_id == topic_id) & (merge_preds.content_id.isin(fp))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6397b00-c715-4466-aaa7-555ab76b48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df[topic_df.id == topic_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab1e73d-07c4-4698-9269-370738a80990",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df[content_df.id.isin(fp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f1cae-2294-416d-a923-077414385402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a8083-2c73-44ce-9cab-c300bd6cfd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63ea1d-a257-4b16-87ef-317492c3d0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9ca573-0809-4e4a-b73e-e08fe6a01b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42471728-7008-41cc-a6d0-870cbfa2767b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26157a-36de-4958-9a1e-2de74ae71d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810cd49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83fb070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gt sims\n",
    "# list_all_content_ids = list(all_content_ids)\n",
    "\n",
    "# all_sims = []\n",
    "# for i, row in tqdm(gt.iterrows()):\n",
    "#     sims = []\n",
    "\n",
    "#     t_e = topic_embs[all_test_ids.index(row[\"topic_id\"])]\n",
    "#     for content_id in row[\"content_ids\"].split(\" \"):\n",
    "#         c_e = content_embs[list_all_content_ids.index(content_id)]\n",
    "#         sims.append(cosine_similarity(torch.from_numpy(t_e), torch.from_numpy(c_e), 0))\n",
    "    \n",
    "#     all_sims.append(\" \".join([str(s.item())[:5] for s in sims]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1228aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prediction sims\n",
    "\n",
    "# list_all_content_ids = list(all_content_ids)\n",
    "\n",
    "# all_preds_sims = []\n",
    "# for i, row in tqdm(preds.iterrows()):\n",
    "#     sims = []\n",
    "\n",
    "#     t_e = topic_embs[all_test_ids.index(row[\"topic_id\"])]\n",
    "#     if not row[\"content_ids\"]:\n",
    "#         continue\n",
    "#     for content_id in row[\"content_ids\"].split(\" \"):\n",
    "#         c_e = content_embs[list_all_content_ids.index(content_id)]\n",
    "#         sims.append(cosine_similarity(torch.from_numpy(t_e), torch.from_numpy(c_e), 0))\n",
    "    \n",
    "#     all_preds_sims.append(\" \".join([str(s.item())[:5] for s in sims]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b988d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_preds_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e9b54f-c4d3-48be-bb7f-80eaa30e3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# sample_preds = pd.DataFrame({\n",
    "#     \"content_ids\": [\n",
    "#         \"a b c\"\n",
    "#     ]\n",
    "# })\n",
    "\n",
    "# sample_gts = pd.DataFrame({\n",
    "#     \"content_ids\": [\n",
    "#         \"a d e f g h\"\n",
    "#     ]\n",
    "# })\n",
    "# f2_score(\n",
    "#     sample_gts[\"content_ids\"],\n",
    "#     sample_preds[\"content_ids\"],\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
