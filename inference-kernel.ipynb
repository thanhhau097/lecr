{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d98797d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bafd401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 30 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, default_collate, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, TrainerCallback\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, default_collate\n",
    "from utils import concat_topic_texts, concat_content_texts\n",
    "import torch\n",
    "from dataset import init_tokenizer\n",
    "import gc\n",
    "from model import Model\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils import clean_text, f2_score, get_pos_score\n",
    "from model import Model\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import os\n",
    "from model import Model\n",
    "import torch.nn as nn\n",
    "from transformers import HfArgumentParser, TrainingArguments\n",
    "from data_args import DataArguments\n",
    "from model_args import ModelArguments\n",
    "import cupy as cp\n",
    "from cuml.metrics import pairwise_distances\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "from pandarallel import pandarallel\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "pandarallel.initialize(nb_workers=30)\n",
    "NUM_WORKERS = 30\n",
    "BATCH_SIZE = 128\n",
    "TEST_MODE = False\n",
    "from optimize_f2 import optimize_cosine_thresh, optimize_xgb_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f31ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"./data/supervised_correlations.csv\")\n",
    "data_df = data_df[[\"topic_id\", \"fold\"]].drop_duplicates().reset_index(drop=True)\n",
    "data_folder = Path(\"./data\")\n",
    "\n",
    "content_df = pd.read_csv(data_folder / \"content.csv\")\n",
    "topic_df = pd.read_csv(data_folder / \"topics.csv\")\n",
    "subs_df = pd.read_csv(data_folder / \"sample_submission.csv\")\n",
    "corrs_df = pd.read_csv(data_folder / \"correlations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a039a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text data for topics\n",
      "Cleaning text data for content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76972it [00:04, 15708.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# parser = HfArgumentParser((TrainingArguments, DataArguments, ModelArguments))\n",
    "# training_args, data_args, model_args = parser.parse_args_into_dataclasses()\n",
    "# training_args: TrainingArguments\n",
    "# data_args: DataArguments\n",
    "# model_args: ModelArguments\n",
    "\n",
    "# Fillna titles\n",
    "topic_df[\"title\"].fillna(\"\", inplace=True)\n",
    "content_df[\"title\"].fillna(\"\", inplace=True)\n",
    "# Fillna descriptions\n",
    "topic_df[\"description\"].fillna(\"\", inplace=True)\n",
    "content_df[\"description\"].fillna(\"\", inplace=True)\n",
    "content_df[\"text\"].fillna(\"\", inplace=True)\n",
    "# clean text\n",
    "print(\"Cleaning text data for topics\")\n",
    "topic_df[\"title\"] = topic_df[\"title\"].parallel_apply(clean_text)\n",
    "topic_df[\"description\"] = topic_df[\"description\"].parallel_apply(clean_text)\n",
    "print(\"Cleaning text data for content\")\n",
    "content_df[\"title\"] = content_df[\"title\"].parallel_apply(clean_text)\n",
    "content_df[\"description\"] = content_df[\"description\"].parallel_apply(clean_text)\n",
    "# parent and children information\n",
    "parents = defaultdict(lambda: [])\n",
    "children = defaultdict(lambda: [])\n",
    "topic_title_dict = {}\n",
    "all_topic_ids = set(topic_df.id.values)\n",
    "for i, row in tqdm(topic_df.iterrows()):\n",
    "    if row[\"parent\"] in all_topic_ids:\n",
    "        parents[row[\"id\"]].append(row[\"parent\"])\n",
    "        children[row[\"parent\"]].append(row[\"id\"])\n",
    "    topic_title_dict[row[\"id\"]] = row[\"title\"]\n",
    "\n",
    "topic_df[\"length\"] = topic_df.apply(\n",
    "    lambda x: len(x[\"title\"] + x[\"description\"]), axis=1\n",
    ")\n",
    "topic_df = topic_df.sort_values(\"length\").reset_index(drop=True)\n",
    "content_df[\"length\"] = content_df.apply(\n",
    "    lambda x: len(x[\"title\"] + x[\"description\"]), axis=1\n",
    ")\n",
    "content_df = content_df.sort_values(\"length\").reset_index(drop=True)\n",
    "\n",
    "topic_ids = topic_df[\"id\"].values\n",
    "content_ids = content_df[\"id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2cf3af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(inputs):\n",
    "    inputs = default_collate(inputs)\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:, :mask_len]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TopicDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = concat_topic_texts(\n",
    "            row, topic_title_dict, parents, children, self.tokenizer.sep_token\n",
    "        )\n",
    "        # topic\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            return_tensors=None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class ContentDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = concat_content_texts(row)\n",
    "        # topic\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            return_tensors=None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast(True)\n",
    "def extract_embs(\n",
    "    tokenizer_name, weights_path, contents_df, topics_df, num_workers=2, batch_size=64\n",
    "):\n",
    "    tokenizer = init_tokenizer(tokenizer_name)\n",
    "\n",
    "    model = Model(tokenizer_name, tokenizer_name, \"siamese\", True)\n",
    "    model = model.cuda()\n",
    "    state_dict = torch.load(weights_path)\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.load_state_dict(state_dict)\n",
    "    else:\n",
    "        model.load_state_dict(state_dict)\n",
    "    print(\"Loaded \", weights_path)\n",
    "    del state_dict\n",
    "    gc.collect()\n",
    "    model.eval()\n",
    "\n",
    "    topics_dataset = TopicDataset(topics_df, tokenizer, max_len=128)\n",
    "    topics_dataloader = DataLoader(\n",
    "        topics_dataset,\n",
    "        num_workers=num_workers,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    topics_embs = []\n",
    "    for inputs in tqdm(topics_dataloader):\n",
    "        inputs = {k: v.cuda(non_blocking=True) for k, v in inputs.items()}\n",
    "        out = model.feature(inputs)\n",
    "        topics_embs.append(out.cpu().numpy())\n",
    "    topics_embs = np.concatenate(topics_embs)\n",
    "    del topics_dataset, topics_dataloader\n",
    "    gc.collect()\n",
    "\n",
    "    contents_embs = []\n",
    "    if len(contents_df) > 0:\n",
    "        contents_dataset = ContentDataset(contents_df, tokenizer, max_len=128)\n",
    "        contents_dataloader = DataLoader(\n",
    "            contents_dataset,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "        for inputs in tqdm(contents_dataloader):\n",
    "            inputs = {k: v.cuda(non_blocking=True) for k, v in inputs.items()}\n",
    "            out = model.feature(inputs)\n",
    "            contents_embs.append(out.cpu().numpy())\n",
    "        del contents_dataset, contents_dataloader\n",
    "        gc.collect()\n",
    "        contents_embs = np.concatenate(contents_embs)\n",
    "\n",
    "    del model, tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return contents_embs, topics_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c927e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cfg = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"learning_rate\": 1e-2,\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"nthread\": os.cpu_count(),\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"gpu_id\": 0\n",
    "}\n",
    "fit_params = {\n",
    "    \"verbose\": False,\n",
    "    \"eval_metric\": \"auc\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4f9c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_names = [\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"sentence-transformers/paraphrase-xlm-r-multilingual-v1\",\n",
    "    \"sentence-transformers/all-roberta-large-v1\",\n",
    "    # \"sentence-transformers/distiluse-base-multilingual-cased\",\n",
    "]\n",
    "weights_folders = [\n",
    "    \"./mpnet-finetune\", \n",
    "    \"./minilm\",\n",
    "    \"./xlm-r\",\n",
    "    \"./roberta-large\" \n",
    "    # \"./distiluse\",\n",
    "]\n",
    "\n",
    "if len(tokenizer_names) == 1:\n",
    "    xgb_path = \"xgb\" + tokenizer_names[0].split(\"/\")[-1]\n",
    "elif len(tokenizer_names) == 2:\n",
    "    xgb_path = \"xgb-mpnet-minilm\"\n",
    "elif len(tokenizer_names) == 3:\n",
    "    xgb_path = \"xgb-mpnet-minilm-xlmr\"\n",
    "elif len(tokenizer_names) == 4:\n",
    "    xgb_path = \"xgb-mpnet-minilm-xlmr-robertalarge\"\n",
    "\n",
    "topk = 50\n",
    "folds = [0, 1, 2, 3, 4, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88082a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgb_features(row, dims):\n",
    "    topic_emb = topic_id_to_emb[row.topic_id]\n",
    "    content_emb = content_id_to_emb[row.content_id]\n",
    "    cur_dim = 0\n",
    "    dists = []\n",
    "    for d in dims:\n",
    "        dists.append(dot(topic_emb[cur_dim:cur_dim+d], content_emb[cur_dim:cur_dim+d]))\n",
    "        cur_dim += d\n",
    "    dists.append(row[\"target\"])    \n",
    "    dists = np.array(dists)\n",
    "    # return dists[None, :]\n",
    "    feature = np.concatenate([topic_emb, content_emb, dists])\n",
    "    return feature[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a807fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm xgb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bafac54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "FOLD  0\n",
      "Loaded  ./mpnet-finetune-fold0/content-embs.pth\n",
      "Loaded  ./mpnet-finetune-fold0/topic-embs.pth\n",
      "Loaded  ./minilm-fold0/content-embs.pth\n",
      "Loaded  ./minilm-fold0/topic-embs.pth\n",
      "Loaded  ./xlm-r-fold0/content-embs.pth\n",
      "Loaded  ./xlm-r-fold0/topic-embs.pth\n",
      "Loaded  ./roberta-large-fold0/content-embs.pth\n",
      "Loaded  ./roberta-large-fold0/topic-embs.pth\n",
      "Training KNN model ...\n",
      "Recall: 0.93572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6152it [00:00, 11390.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost inference time: 1.0979394912719727 seconds.\n",
      "====================================================================================================\n",
      "FOLD  1\n",
      "Loaded  ./mpnet-finetune-fold1/content-embs.pth\n",
      "Loaded  ./mpnet-finetune-fold1/topic-embs.pth\n",
      "Loaded  ./minilm-fold1/content-embs.pth\n",
      "Loaded  ./minilm-fold1/topic-embs.pth\n",
      "Loaded  ./xlm-r-fold1/content-embs.pth\n",
      "Loaded  ./xlm-r-fold1/topic-embs.pth\n",
      "Loaded  ./roberta-large-fold1/content-embs.pth\n",
      "Loaded  ./roberta-large-fold1/topic-embs.pth\n",
      "Training KNN model ...\n",
      "Recall: 0.93233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6152it [00:00, 11336.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost inference time: 1.1615498065948486 seconds.\n",
      "====================================================================================================\n",
      "FOLD  2\n",
      "Loaded  ./mpnet-finetune-fold2/content-embs.pth\n",
      "Loaded  ./mpnet-finetune-fold2/topic-embs.pth\n",
      "Loaded  ./minilm-fold2/content-embs.pth\n",
      "Loaded  ./minilm-fold2/topic-embs.pth\n",
      "Loaded  ./xlm-r-fold2/content-embs.pth\n",
      "Loaded  ./xlm-r-fold2/topic-embs.pth\n",
      "Loaded  ./roberta-large-fold2/content-embs.pth\n",
      "Loaded  ./roberta-large-fold2/topic-embs.pth\n",
      "Training KNN model ...\n",
      "Recall: 0.93519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6152it [00:00, 11478.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost inference time: 1.1272263526916504 seconds.\n",
      "====================================================================================================\n",
      "FOLD  3\n",
      "Loaded  ./mpnet-finetune-fold3/content-embs.pth\n",
      "Loaded  ./mpnet-finetune-fold3/topic-embs.pth\n",
      "Loaded  ./minilm-fold3/content-embs.pth\n",
      "Loaded  ./minilm-fold3/topic-embs.pth\n",
      "Loaded  ./xlm-r-fold3/content-embs.pth\n",
      "Loaded  ./xlm-r-fold3/topic-embs.pth\n",
      "Loaded  ./roberta-large-fold3/content-embs.pth\n",
      "Loaded  ./roberta-large-fold3/topic-embs.pth\n",
      "Training KNN model ...\n",
      "Recall: 0.93215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6152it [00:00, 11302.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost inference time: 1.0869686603546143 seconds.\n",
      "====================================================================================================\n",
      "FOLD  4\n",
      "Loaded  ./mpnet-finetune-fold4/content-embs.pth\n",
      "Loaded  ./mpnet-finetune-fold4/topic-embs.pth\n",
      "Loaded  ./minilm-fold4/content-embs.pth\n",
      "Loaded  ./minilm-fold4/topic-embs.pth\n",
      "Loaded  ./xlm-r-fold4/content-embs.pth\n",
      "Loaded  ./xlm-r-fold4/topic-embs.pth\n",
      "Loaded  ./roberta-large-fold4/content-embs.pth\n",
      "Loaded  ./roberta-large-fold4/topic-embs.pth\n",
      "Training KNN model ...\n",
      "Recall: 0.93028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6152it [00:00, 11042.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost inference time: 1.1136953830718994 seconds.\n",
      "====================================================================================================\n",
      "FOLD  7\n",
      "Loaded  ./mpnet-finetune-fold7/content-embs.pth\n",
      "Loaded  ./mpnet-finetune-fold7/topic-embs.pth\n",
      "Loaded  ./minilm-fold7/content-embs.pth\n",
      "Loaded  ./minilm-fold7/topic-embs.pth\n",
      "Loaded  ./xlm-r-fold7/content-embs.pth\n",
      "Loaded  ./xlm-r-fold7/topic-embs.pth\n",
      "Loaded  ./roberta-large-fold7/content-embs.pth\n",
      "Loaded  ./roberta-large-fold7/topic-embs.pth\n",
      "Training KNN model ...\n",
      "Recall: 0.93122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6152it [00:00, 11482.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost inference time: 1.274050235748291 seconds.\n"
     ]
    }
   ],
   "source": [
    "oof_distances = []\n",
    "oof_indices = []\n",
    "oof_topic_ids = []\n",
    "xgb_preds = []\n",
    "\n",
    "for fold in folds:\n",
    "    print(\"=\" * 100)\n",
    "    print(\"FOLD \", fold)\n",
    "\n",
    "    val_topic_ids = list(data_df[data_df[\"fold\"] == fold].topic_id)\n",
    "    val_topic_df = topic_df[topic_df.id.isin(val_topic_ids)].reset_index(drop=True)\n",
    "    gt = corrs_df[corrs_df.topic_id.isin(val_topic_ids)].sort_values(\"topic_id\")\n",
    "    content_ids = content_df.id.values\n",
    "    \n",
    "    topic_embs = []\n",
    "    content_embs = []\n",
    "    dims = []\n",
    "    \n",
    "    for tokenizer_name, weights_folder in zip(tokenizer_names, weights_folders):\n",
    "        weights_path = weights_folder + f\"-fold{fold}/pytorch_model.bin\"\n",
    "        contents_path = weights_folder + f\"-fold{fold}/content-embs.pth\"\n",
    "        topics_path = weights_folder + f\"-fold{fold}/topic-embs.pth\"\n",
    "        \n",
    "        if os.path.exists(contents_path) and os.path.exists(topics_path):\n",
    "            contents_dict = torch.load(contents_path)\n",
    "            model_content_embs = np.stack(contents_dict.values(), 0)\n",
    "            print(\"Loaded \", contents_path)\n",
    "            topics_dict = torch.load(topics_path)\n",
    "            model_topic_embs = np.stack(topics_dict.values(), 0)\n",
    "            print(\"Loaded \", topics_path)\n",
    "        else:\n",
    "            model_content_embs, model_topic_embs = extract_embs(\n",
    "                tokenizer_name, weights_path, content_df, val_topic_df, NUM_WORKERS, BATCH_SIZE\n",
    "            )\n",
    "            torch.save(\n",
    "                {k: v for k, v in zip(content_ids, model_content_embs)}, contents_path\n",
    "            )\n",
    "            torch.save(\n",
    "                {k: v for k, v in zip(val_topic_df.id.values, model_topic_embs)}, topics_path\n",
    "            )\n",
    "            \n",
    "        topic_embs.append(model_topic_embs)\n",
    "        content_embs.append(model_content_embs)\n",
    "        dims.append(model_content_embs.shape[1])\n",
    "        del model_content_embs, model_topic_embs\n",
    "        gc.collect()\n",
    "\n",
    "    topic_embs = np.concatenate(topic_embs, 1)\n",
    "    content_embs = np.concatenate(content_embs, 1)\n",
    "    topic_id_to_emb = {k: v for k, v in zip(val_topic_df.id.values, topic_embs)}\n",
    "    content_id_to_emb = {k: v for k, v in zip(content_ids, content_embs)}\n",
    "\n",
    "    # Transfer predictions to gpu\n",
    "    topic_embs_gpu = cp.array(topic_embs)\n",
    "    content_embs_gpu = cp.array(content_embs)\n",
    "    print(\"Training KNN model ...\")\n",
    "    neighbors_model = NearestNeighbors(n_neighbors=topk, metric=\"cosine\")\n",
    "    neighbors_model.fit(content_embs_gpu)\n",
    "    distances, indices = neighbors_model.kneighbors(\n",
    "        topic_embs_gpu, return_distance=True\n",
    "    )\n",
    "\n",
    "    del content_embs, topic_embs, topic_embs_gpu, content_embs_gpu, neighbors_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    indices = indices.get()\n",
    "    distances = distances.get()\n",
    "    predictions = []\n",
    "    for k in range(len(indices)):\n",
    "        pred = indices[k]\n",
    "        pred = content_ids[pred]\n",
    "        assert pred.shape[0] == topk\n",
    "        predictions.append(\" \".join(pred))\n",
    "    knn_preds = pd.DataFrame(\n",
    "        {\n",
    "            \"topic_id\": val_topic_df.id.values,\n",
    "            \"content_ids\": predictions,\n",
    "        }\n",
    "    ).sort_values(\"topic_id\")\n",
    "    recall = get_pos_score(gt[\"content_ids\"], knn_preds[\"content_ids\"], topk)\n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "    pairs = set()\n",
    "    gt_dict = gt.set_index(\"topic_id\")[\"content_ids\"].apply(lambda x: set(x.split(\" \"))).to_dict()\n",
    "    for i, row in tqdm(knn_preds.iterrows()):\n",
    "        topic_id = row.topic_id\n",
    "        for content_id in row.content_ids.split(\" \"):\n",
    "            if content_id in gt_dict[topic_id]:\n",
    "                pairs.add((topic_id, content_id, 1))\n",
    "            else:\n",
    "                pairs.add((topic_id, content_id, 0))\n",
    "    \n",
    "    val_df = pd.DataFrame({\n",
    "        \"topic_id\": [pair[0] for pair in pairs],\n",
    "        \"content_id\": [pair[1] for pair in pairs],\n",
    "        \"target\": [pair[2] for pair in pairs],\n",
    "    })\n",
    "    val_df = val_df.sort_values(\"topic_id\").reset_index(drop=True)\n",
    "    \n",
    "    # split val_topic_ids\n",
    "    train_ids, val_ids = train_test_split(val_topic_ids, test_size=0.2, random_state=42)\n",
    "    train_idxs = val_df[val_df.topic_id.isin(train_ids)].index.values\n",
    "    val_idxs = val_df[val_df.topic_id.isin(val_ids)].index.values    \n",
    "\n",
    "    # xgboost\n",
    "    features = np.concatenate(val_df.parallel_apply(lambda x: get_xgb_features(x, dims), axis=1).values)\n",
    "    xgb_ckpt_path = f\"{xgb_path}-fold{fold}\"        \n",
    "    xgb = XGBClassifier(**xgb_cfg)\n",
    "    if os.path.exists(xgb_ckpt_path):\n",
    "        xgb.load_model(xgb_ckpt_path)\n",
    "    else:\n",
    "        x_train, y_train = features[train_idxs][:, :-1], features[train_idxs][:, -1]\n",
    "        x_valid, y_valid = features[val_idxs][:, :-1], features[val_idxs][:, -1]    \n",
    "        start = time.time()\n",
    "        xgb.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], **fit_params)\n",
    "        print(f\"xgboost training time: {time.time() - start} seconds.\")\n",
    "        xgb.save_model(xgb_ckpt_path)\n",
    "    start = time.time()\n",
    "    val_df[\"ensemble_score\"] = xgb.predict_proba(\n",
    "        features[:, :-1], ntree_limit=xgb.best_ntree_limit)[:, 1]\n",
    "    print(f\"xgboost inference time: {time.time() - start} seconds.\")\n",
    "\n",
    "    val_df = val_df.sort_values(\"ensemble_score\").reset_index(drop=True)\n",
    "    xgb_preds.append(val_df[val_df.topic_id.isin(val_ids)])\n",
    "        \n",
    "    oof_indices.append(indices)\n",
    "    oof_distances.append(distances)\n",
    "    oof_topic_ids.append(val_topic_df.id.values)\n",
    "\n",
    "oof_distances = np.concatenate(oof_distances, 0)\n",
    "oof_indices = np.concatenate(oof_indices, 0)\n",
    "oof_topic_ids = np.concatenate(oof_topic_ids, 0)\n",
    "xgb_preds = pd.concat(xgb_preds, 0)\n",
    "val_topic_ids = xgb_preds[\"topic_id\"].unique()\n",
    "val_gt = corrs_df[corrs_df.topic_id.isin(val_topic_ids)].sort_values(\"topic_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "133ed82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 12.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7082, 0.718, 0.723, 0.7276, 0.7308, 0.7325, 0.7336, 0.7334, 0.732, 0.7314, 0.7312, 0.7307, 0.7294, 0.7287, 0.7276, 0.7267]\n",
      "Xgb threshold: 0.11000000000000001 - Xgb score: 0.7336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine threshold: 0.3100000000000001 - Cosine score: 0.7217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_thresh, best_xgb_preds = optimize_xgb_thresh(xgb_preds, val_gt, np.arange(0.05, 0.2, 0.01))\n",
    "xgb_score = f2_score(val_gt[\"content_ids\"], best_xgb_preds[\"content_ids\"])\n",
    "print(f\"Xgb threshold: {xgb_thresh} - Xgb score: {xgb_score}\")\n",
    "\n",
    "cosine_thresh, cosine_preds = optimize_cosine_thresh(\n",
    "    oof_distances, oof_indices, oof_topic_ids,\n",
    "    corrs_df[corrs_df.topic_id.isin(oof_topic_ids)].sort_values(\"topic_id\"), \n",
    "    np.arange(0.2, 0.35, 0.01), content_ids)\n",
    "cosine_score = f2_score(val_gt[\"content_ids\"], \n",
    "                        cosine_preds[cosine_preds.topic_id.isin(val_topic_ids)].sort_values(\"topic_id\")[\"content_ids\"])\n",
    "print(f\"Cosine threshold: {cosine_thresh} - Cosine score: {cosine_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9fb987bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE\t\t\t     roberta-large-fold0\n",
      "README.md\t\t     roberta-large-fold1\n",
      "Untitled.ipynb\t\t     roberta-large-fold2\n",
      "__pycache__\t\t     roberta-large-fold3\n",
      "csv_samplers.py\t\t     roberta-large-fold4\n",
      "data\t\t\t     roberta-large-fold5\n",
      "data_args.py\t\t     roberta-large-fold6\n",
      "dataset.py\t\t     roberta-large-fold7\n",
      "dataset_callback.py\t     roberta.zip\n",
      "distiluse\t\t     samplers.py\n",
      "distiluse-fold0\t\t     tokenizer.py\n",
      "distiluse-fold1\t\t     train.py\n",
      "distiluse-fold2\t\t     train_reranker.py\n",
      "distiluse-fold3\t\t     utils.py\n",
      "distiluse-fold4\t\t     xgb-mpnet-minilm-distiluse-xlmr-fold0\n",
      "distiluse-fold5\t\t     xgb-mpnet-minilm-distiluse-xlmr-fold1\n",
      "engine.py\t\t     xgb-mpnet-minilm-distiluse-xlmr-fold2\n",
      "ensembling-retrievers.ipynb  xgb-mpnet-minilm-xlmr-fold0\n",
      "evaluate.ipynb\t\t     xgb-mpnet-minilm-xlmr-fold1\n",
      "inference.py\t\t     xgb-mpnet-minilm-xlmr-fold2\n",
      "losses.py\t\t     xgb-mpnet-minilm-xlmr-fold3\n",
      "minilm-fold0\t\t     xgb-mpnet-minilm-xlmr-fold4\n",
      "minilm-fold1\t\t     xgb-mpnet-minilm-xlmr-fold7\n",
      "minilm-fold2\t\t     xgb-mpnet-minilm-xlmr-robertalarge-fold0\n",
      "minilm-fold3\t\t     xgb-mpnet-minilm-xlmr-robertalarge-fold1\n",
      "minilm-fold4\t\t     xgb-mpnet-minilm-xlmr-robertalarge-fold2\n",
      "minilm-fold7\t\t     xgb-mpnet-minilm-xlmr-robertalarge-fold3\n",
      "model.py\t\t     xgb-mpnet-minilm-xlmr-robertalarge-fold4\n",
      "model_args.py\t\t     xgb-mpnet-minilm-xlmr-robertalarge-fold7\n",
      "mpnet-finetune-fold0\t     xgbparaphrase-multilingual-mpnet-base-v2-fold0\n",
      "mpnet-finetune-fold1\t     xgbparaphrase-multilingual-mpnet-base-v2-fold1\n",
      "mpnet-finetune-fold2\t     xgbparaphrase-multilingual-mpnet-base-v2-fold2\n",
      "mpnet-finetune-fold3\t     xgbparaphrase-multilingual-mpnet-base-v2-fold3\n",
      "mpnet-finetune-fold4\t     xgbparaphrase-multilingual-mpnet-base-v2-fold4\n",
      "mpnet-finetune-fold5\t     xgbparaphrase-multilingual-mpnet-base-v2-fold7\n",
      "mpnet-finetune-fold6\t     xlm-r\n",
      "mpnet-finetune-fold7\t     xlm-r-fold0\n",
      "mpnet-fold8\t\t     xlm-r-fold1\n",
      "mpnet-fold9\t\t     xlm-r-fold2\n",
      "optimize_f2.py\t\t     xlm-r-fold3\n",
      "requirements.txt\t     xlm-r-fold4\n",
      "rerank_dataset.py\t     xlm-r-fold7\n",
      "rerank_engine.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff645dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
